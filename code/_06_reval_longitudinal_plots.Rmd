---
title: "04_longitudinal"
author: "Veronica Mandelli"
date: "2/14/2022"
output: html_document
---

This script run the longitudinal model (statistics + graphics) for VABS and MSEL for the 2 ASD subtypes identified wiht NDA database usign the reval algorithm

```{r}
# load libraries
library(grid)
library(lattice)
library(ggplot2)
library(reshape2)
library(gridExtra)
library(grid)
library(openxlsx)

#set paths
work_dir = "..."
codepath = file.path(work_dir,"code")
data_long_path = file.path(work_dir,"results","data_wrangling_output")
phenopath = file.path(work_dir,"ndar_datawrangling_","nda_rawdata")
source(file.path(codepath,"spaghettiPlot2.R"))
color = c("#FF9933","#3399CC","#009900","#CC0099") 

# with age as input feature
datapath = file.path(work_dir,"results","reval_AGE")
resultpath = file.path(work_dir,"results","reval_AGE")
plot_path = file.path(work_dir,"plot","reval_AGE")
file_name= "dataset_reval_clusters_oct23_AGE_grid_search.csv"
data_long_path = file.path(work_dir,"results","data_wrangling_output")
```

### reval output (T1)
```{r}
df_T1 = read.csv(file.path(datapath,file_name))

# set names of clusters 
data = df_T1
for (i in (1:length(data$cluster_domain))) {
    if (data[i, 'cluster_domain'] == '1'){
      data[i,'clust_dom'] = "high"
    }
    else if (data[i, 'cluster_domain'] == '0'){
      data[i,'clust_dom'] = "low"
    }
    
}

df_T1=  data
df_T1$clust_dom <- factor( df_T1$clust_dom,levels = c('high','low'))

# ## get info about the collection ID to insert as random factor in lme ## NOOO collection ID already present!!!
# file_name = "ndar_phenotypes.txt"
# pheno = read.delim(file.path(phenopath,file_name))
# sublist  = df_T1$subjectkey
# 
# pheno_sub = as.data.frame(pheno[pheno$subjectkey %in% sublist,c("subjectkey","collection_id")])
# pheno_sub_nodup = pheno_sub[!duplicated(pheno_sub),]


# save sub_ids + collection ID
dfT12save = df_T1[,c('subjectkey', "collection_id","interview_age_VABS","interview_age_MELS") ]
write.csv(dfT12save,file.path(resultpath,"sub_id_collectionid_final.csv"))
sublist  = df_T1$subjectkey
```

### VABS merge longitudinal data
```{r}
# VABS longitudinal

# load data
file = 'vineland_longitudinal_oct23.txt'
data_long_vabs = read.csv(file.path(data_long_path,file))

# change age col name
data_long_vabs$interview_age_vabs = data_long_vabs$interview_age

# subset col to use
col2use_vabs = c("subjectkey","interview_age_vabs", "communicationdomain_totalb","livingskillsdomain_totalb",
           "socializationdomain_totalb","motorskillsdomain_totalb")
data_long_vabs = data_long_vabs[,col2use_vabs] 

# select subjects_ID used in this project
data_long_vabs_sub = data_long_vabs[data_long_vabs$subjectkey %in% sublist,]
print(paste ('VABS longitudinal data point:',length(data_long_vabs_sub$interview_age_vabs)))


# drop duplicated observation
# 1) drop complete duplicated
data_long_vabs_sub = data_long_vabs_sub[!duplicated(data_long_vabs_sub),]
print(length(data_long_vabs_sub$interview_age_vabs))
# 2) drop duplicated when in the second observation (same subject and same age) there is only one measure
# 2.1) create a column with sum of NANS
data_long_vabs_sub$sum_na = rowSums(is.na(data_long_vabs_sub))
# 2.2) change the order based on sub_id and missing
data_long_vabs_sub = data_long_vabs_sub[order(data_long_vabs_sub[,'subjectkey'],data_long_vabs_sub[,'interview_age_vabs'], data_long_vabs_sub[,'sum_na']), ]
# 2.3) drop the subject with more nans
data_long_vabs_sub = data_long_vabs_sub[!duplicated(data_long_vabs_sub[,1:2]),]
print(length(data_long_vabs_sub$interview_age_vabs))

# total number of subject  
print(paste ('VABS longitudinal individual subjects:',length(unique(data_long_vabs_sub$subjectkey))))

# merge with cluster label
data_long_vabs_clust = merge(data_long_vabs_sub,df_T1[,c("subjectkey","clust_dom","collection_id")])
```

# VABS plot
```{r}
#var to use
var2use = c("communicationdomain_totalb","livingskillsdomain_totalb",
           "socializationdomain_totalb","motorskillsdomain_totalb")
data2plot = data_long_vabs_clust
size  = 3

#prepare for the spaghetti
data_long_vabs_clust$cluster = data_long_vabs_clust$clust_dom
data_long_vabs_clust$cluster=factor(data_long_vabs_clust$cluster,levels = c('high','low'))#c("high","med","low"))
data_long_vabs_clust$'subjectId'=data_long_vabs_clust$subjectkey
#data_long_vabs_clust$dataset = data_long_vabs_clust$collection_id
data_long_vabs_clust$dataset = as.factor(data_long_vabs_clust$collection_id)

plt_vabs = list()
x_lim = list(c(0, 100),c(0, 100),c(0, 100),c(0, 84))
title_list = c("Communication","Daily Living Skills","Socialization", "Motor" )

for (i in 1:length(var2use)) {
  col2use =var2use[i]
  x_lim_val = x_lim[[i]]
  title = title_list[i]
  resp = spaghettiPlot2(
                data2use = data_long_vabs_clust,
                x_var = 'interview_age_vabs',
                y_var = col2use,
                subgrp_var = "cluster",
                cov_vars=NULL,
                xLabel = 'Age',
                yLabel = "Standardized score",
                modelType = "linear",
                fname2save = NULL,
                plot_dots = FALSE, plot_lines = TRUE,
                dot_alpha = 1/10, line_alpha = 3/10,
                xLimits = x_lim_val, yLimits = c(20,125),
                lineColor = "cluster", dotColor = "cluster",
                legend_name = "Slope", maxIter = 500,
                plot_title = title
                )
 
  # change color 
  #change color 
  cols2use = color
  font_size = 11
  p_com = resp$p
  p_com = p_com + scale_colour_manual(values = cols2use) + scale_fill_manual(values = cols2use)
  p_com = p_com + theme_bw(base_size = font_size) +
  theme(plot.title = element_text(hjust = 0.5, size=font_size)) +
  guides(colour = FALSE, fill = FALSE)
  
   plt_vabs[[i]]=p_com
   print(p_com)
   
   # do lme analysis
  lme2save = anova(resp$lme_model)
  # correct p value for multiple comparison
  for (a in 1:3){
    p = lme2save[a,"Pr(>F)"]
    #print('p')
    #print(p)
    p_adj_ = p.adjust(p, method = "fdr", n = size)
    #print('p_adj_')
    #print(p_adj_)
    lme2save[a,'p_adj_temp'] = p_adj_
  }
  lme_final_2save = lme2save[,c("Sum Sq","Mean Sq","NumDF","DenDF","F value","Pr(>F)","p_adj_temp")]
  colnames(lme_final_2save)= c("Sum Sq","Mean Sq","NumDF","DenDF","F value","p value","p_adj")                      
  #write.xlsx(lme_final_2save, file.path(resultpath,paste('Long_',title,"_new_oct23_AGE.xlsx",sep = '')))
  
  
}
```

### MSEL merge longitudinal data
```{r}
# MSEL longitudinal data load and prepare foe entering in the longitudinal analaysis
file = 'mullen_longitudinal_oct23.txt'
data_long_mels = read.csv(file.path(data_long_path,file))

#sub_list
mels_sub_list  = data_long_mels$subjectkey
cols_age_eq = c( "scoresumm_vr_age_equiv", "scoresumm_fm_age_equiv",
                "scoresumm_rl_age_equiv",  "scoresumm_el_age_equiv")
cols_corr_T = c("DQ_VR", "DQ_FM",
               "DQ_RL",  "DQr_EL")

# transform the age equivalent into DQ also for the longitudinal data
for (row in (1:length(data_long_mels$subjectkey))){
  for (feature in 1:4 ){
    eq_col = cols_age_eq[feature]
    T_corr_col = cols_corr_T[feature]
    #T_original = cols_t[feature]
    if (!is.na(data_long_mels[row,eq_col])){
    data_long_mels[row,T_corr_col]= round(((data_long_mels[row,eq_col]/data_long_mels[row,'interview_age'])*100))}
    }
}

# for (col in cols_corr_T ){
# hist = hist(data_long_mels[,col], main = col)
# hist}

# change name to the age column
data_long_mels$interview_age_mels= data_long_mels$interview_age
cols2use_mels = c("subjectkey","interview_age_mels",cols_corr_T)
print(length(data_long_mels$interview_age_mels))

# select subjects_ID used in this project
data_long_mels_sub = data_long_mels[data_long_mels$subjectkey %in% sublist,]
print(length(data_long_mels_sub$interview_age_mels))

# drop duplciated observation
# 1) drop complete duplicated
data_long_mels_sub = data_long_mels_sub[!duplicated(data_long_mels_sub),]
print(length(data_long_mels_sub$subjectkey))

# 2) drop duplicated when in the second observation (same subject and age) there is only there is only measure 
# 2.1) create a column with sum of NANS
data_long_mels_sub$sum_na = rowSums(is.na(data_long_mels_sub))
# 2.2.) change the oreder based on sub_id and missing
data_long_mels_sub = data_long_mels_sub[order(data_long_mels_sub[,'subjectkey'],data_long_mels_sub[,'interview_age_mels'], data_long_mels_sub[,'sum_na']), ]
# 2.3) drop the subject with more nans
data_long_mels_sub = data_long_mels_sub[!duplicated(data_long_mels_sub[,1:2]),]
print(length(data_long_mels_sub$interview_age_mels))

# total number of subject 
print(paste ('MSEL longitudinal individual subjects:',length(unique(data_long_mels_sub$subjectkey))))

# take only subject <68 monhts 
data_long_mels_clust = data_long_mels_sub[data_long_mels_sub$interview_age_mels<=68,]

# merge the clusters
data_long_mels_clust=merge(data_long_mels_clust,df_T1[,c("subjectkey","clust_dom","collection_id")])
```

### MSEL plot
```{r}
#var to use
#var2use =c("scoresumm_vr_t_score" ,   "scoresumm_fm_t_score" ,   "scoresumm_rl_t_score"   , "scoresumm_el_t_score") #cols_age_eq
var2use = cols_age_eq
data2plot = data_long_mels_clust
size = 3

# prepare for the spaghetti
data_long_mels_clust$cluster = data_long_mels_clust$clust_dom
data_long_mels_clust$cluster = factor(data_long_mels_clust$cluster,levels = c("high","low"))
data_long_mels_clust$'subjectId' = data_long_mels_clust$subjectkey
data_long_mels_clust$dataset = data_long_mels_clust$collection_id
data_long_mels_clust$dataset = as.factor(data_long_mels_clust$dataset)
title_list  = c("Visual Reception","Fine Motor","Receptive Language","Expressive Language")
plt_mels = list()

for (i in 1:length(var2use)) {
  col2use =var2use[i]
  title= title_list[i]
  resp = spaghettiPlot2(
                data2use = data_long_mels_clust,
                x_var = 'interview_age_mels',
                y_var = col2use,
                subgrp_var = "cluster",
                cov_vars=NULL,
                xLabel = 'Age',
                yLabel = "Age equivalent",
                modelType = "linear",
                fname2save = NULL,
                plot_dots = FALSE, plot_lines = TRUE,
                dot_alpha = 1/10, line_alpha = 3/10,
                xLimits = c(0, 60), yLimits = c(0, 90),
                lineColor = "cluster", dotColor = "cluster",
                legend_name = "Slope", maxIter = 500,
                plot_title = title,
                )
  
  
  #change color 
  cols2use = color
  font_size = 11
  p_com = resp$p
  p_com = p_com + scale_colour_manual(values = cols2use) + scale_fill_manual(values = cols2use)
  p_com = p_com + theme_bw(base_size = font_size) +
  theme(plot.title = element_text(hjust = 0.5, size=font_size)) +
  guides(colour = FALSE, fill = FALSE)
 
 plt_mels[[i]]=p_com
 print(p_com)
  
 # do lme analysis
  lme2save = anova(resp$lme_model)
  # correct p value for multiple comparison
  for (a in 1:3){
    p = lme2save[a,"Pr(>F)"]
    #print('p')
    #print(p)
    p_adj_ = p.adjust(p, method = "fdr", n = size)
    #print('p_adj_')
    #print(p_adj_)
    lme2save[a,'p_adj_temp'] = p_adj_
  }
  lme_final_2save = lme2save[,c("Sum Sq","Mean Sq","NumDF","DenDF","F value","Pr(>F)","p_adj_temp")]
  colnames(lme_final_2save)= c("Sum Sq","Mean Sq","NumDF","DenDF","F value","p value","p_adj")                      
  #write.xlsx(lme_final_2save, file.path(resultpath,paste('Long_',title,"_new_oct23_AGE.xlsx",sep = '')))
}
```



```{r}
g = grid.arrange(
  plt_vabs[[1]],
  plt_vabs[[2]],
  plt_vabs[[3]],
  plt_vabs[[4]],
  plt_mels[[1]],
  plt_mels[[2]],
  plt_mels[[3]],
  plt_mels[[4]],
  nrow = 2)
g
ggsave(file.path(plot_path,paste('MSEL_VABS_longitudina_new_oct23_correct.pdf',sep="")), g, width = 10 ,height = 5)
```
# summary table of time point for each instrument
```{r}
#reorder the table for subject and age of msel
data_long_mels_clust = data_long_mels_clust[order(data_long_mels_clust$subjectkey, data_long_mels_clust$interview_age_mels), ]
#assign time frame for each subject
subsetting = as.data.frame(matrix(ncol=ncol(data_long_mels_clust)+1,nrow=0))
colnames(subsetting)=c(colnames(data_long_mels_clust),"TimePoint")

for (sub2use in unique(data_long_mels_clust$subjectkey)){
  subset_ = subset(data_long_mels_clust,data_long_mels_clust$subjectkey==sub2use)
  for (i in 1:length(subset_$subjectkey)){
    subset_[i,'TimePoint']= i
  }
  subsetting = rbind(subsetting,subset_)
}


#make the table 
table_sum_msel=as.data.frame(matrix(ncol=4,nrow=0))
colnames(table_sum_msel)= c('TimePoint','size','mean_age',"sd_age")
for (k in 1: max(unique(subsetting$TimePoint))){
subset_ =  subset(subsetting,subsetting$TimePoint==k)
  table_sum_msel[k,"TimePoint"] = k
  table_sum_msel[k,"size"] = length(unique(subset_$subjectkey))
  table_sum_msel[k,"mean_age"] = mean(subset_$interview_age_mels)
  table_sum_msel[k,"sd_age"] = sd(subset_$interview_age_mels)
}


# VABS

# assign time frame for each subject
subsetting = as.data.frame(matrix(ncol=ncol(data_long_vabs_clust)+1,nrow=0))
colnames(subsetting)=c(colnames(data_long_vabs_clust),"TimePoint")
for (sub2use in unique(data_long_vabs_clust$subjectkey)){
  subset_ = subset(data_long_vabs_clust,data_long_vabs_clust$subjectkey==sub2use)
  for (i in 1:length(subset_$subjectkey)){
    subset_[i,'TimePoint']= i
  }
  subsetting = rbind(subsetting,subset_)
}

#make the table 
table_sum_vabs=as.data.frame(matrix(ncol=4,nrow=0))
colnames(table_sum_vabs)= c('TimePoint','size','mean_age',"sd_age")
for (k in 1: max(unique(subsetting$TimePoint))){
subset_ =  subset(subsetting,subsetting$TimePoint==k)
  table_sum_vabs[k,"TimePoint"] = k
  table_sum_vabs[k,"size"] = length(unique(subset_$subjectkey))
  table_sum_vabs[k,"mean_age"] = mean(subset_$interview_age_vabs)
  table_sum_vabs[k,"sd_age"] = sd(subset_$interview_age_vabs)
}

write.xlsx(table_sum_msel,file.path(resultpath,'time_point_msel_oct23.xlsx'))
write.xlsx(table_sum_vabs,file.path(resultpath,'time_point_vabs_oct23.xlsx'))
```

